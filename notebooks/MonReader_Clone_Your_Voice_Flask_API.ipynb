{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPxud7aZ5wa2TTn3H+iwHmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinSMoyo/2XYDqXDc6wzA716j/blob/main/notebooks/MonReader_Clone_Your_Voice_Flask_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Sanitize notebook metadata so GitHub can preview it\n",
        "# - Finds the notebook under Drive/Colab Notebooks\n",
        "# - Removes metadata.widgets or adds an empty state\n",
        "\n",
        "import nbformat, glob, os, sys\n",
        "\n",
        "NAME = \"MonReader_Clone_Your_Voice_Flask_API.ipynb\"\n",
        "\n",
        "# 1) Mount Drive if not already\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 2) Search common locations\n",
        "cands = []\n",
        "cands += glob.glob(f\"/content/{NAME}\")\n",
        "cands += glob.glob(f\"/content/**/*.ipynb\", recursive=True)\n",
        "cands += glob.glob(f\"/content/drive/MyDrive/**/*.ipynb\", recursive=True)\n",
        "\n",
        "target = None\n",
        "for p in cands:\n",
        "    if os.path.basename(p) == NAME:\n",
        "        target = p\n",
        "        break\n",
        "\n",
        "if not target:\n",
        "    raise FileNotFoundError(f\"Could not find {NAME}. If you saved it elsewhere, set target path manually.\")\n",
        "\n",
        "print(\"Found notebook:\", target)\n",
        "\n",
        "# 3) Load, sanitize, write\n",
        "nb = nbformat.read(target, as_version=4)\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    nb[\"metadata\"].pop(\"widgets\", None)   # safest: drop widgets metadata\n",
        "# Alternatively: add an empty state\n",
        "# nb.setdefault(\"metadata\", {}).setdefault(\"widgets\", {}).setdefault(\"state\", {})\n",
        "\n",
        "nbformat.write(nb, target)\n",
        "print(\"Sanitized and saved:\", target)"
      ],
      "metadata": {
        "id": "2Uap22N5MTce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DqvCi6KkUGZ"
      },
      "outputs": [],
      "source": [
        "# ====================================================================\n",
        "# STEP 1 ‚Äî Definitive Setup (with Dependency Fixes)\n",
        "# ====================================================================\n",
        "\n",
        "# --- 1. Install System Libraries ---\n",
        "print(\"Installing system dependencies...\")\n",
        "# Use -qq to make the output cleaner\n",
        "!apt-get update -qq && apt-get install -y -qq portaudio19-dev ffmpeg\n",
        "\n",
        "# --- 2. Install Python Libraries in a Specific Order ---\n",
        "print(\"\\nInstalling Python libraries...\")\n",
        "\n",
        "# First, clone and install 'csm' which has strict requirements\n",
        "!git clone https://github.com/SesameAILabs/csm.git\n",
        "%cd csm\n",
        "# This will install its required version of huggingface-hub (0.28.1) and torch\n",
        "!pip install -q -e .\n",
        "%cd ..\n",
        "\n",
        "# Next, install gradio WITHOUT its dependencies to avoid the huggingface-hub conflict\n",
        "!pip install -q --no-deps gradio\n",
        "\n",
        "# Finally, install the rest of the required libraries\n",
        "!pip install -q openai groq sounddevice scipy flask flask-cors werkzeug\n",
        "\n",
        "# --- 3. Authenticate with Hugging Face ---\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "print(\"\\nAuthenticating with Hugging Face...\")\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "login(token=HF_TOKEN)\n",
        "print(\"‚úÖ Hugging Face authentication complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 2 ‚Äî Prepare Voices (Mount Drive + Clean Prompt Clips)\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "import os, glob, subprocess, shlex\n",
        "\n",
        "# --- 2.1 Mount Google Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"‚úÖ Google Drive mounted at /content/drive\")\n",
        "\n",
        "# --- 2.2 Clean & Normalize Prompt Clips ---\n",
        "VOICES_DIR = \"/content/drive/MyDrive/My_Voice_API_Files/voices\"\n",
        "TARGET_SR  = 24000  # must match your backend SAMPLE_RATE\n",
        "\n",
        "# Sanity check: voices dir exists?\n",
        "if not os.path.isdir(VOICES_DIR):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Voices folder not found at: {VOICES_DIR}\\n\"\n",
        "        \"‚û°Ô∏è Create it like /My_Voice_API_Files/voices/<VoiceName>/<your_prompt>.wav\"\n",
        "    )\n",
        "\n",
        "# Optional: show ffmpeg version (helps debugging audio issues live)\n",
        "try:\n",
        "    _ = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=False)\n",
        "except Exception:\n",
        "    print(\"‚ö†Ô∏è ffmpeg not found in PATH. Make sure Step 1 installed it.\")\n",
        "\n",
        "print(\"üßº Cleaning & normalizing reference .wav files...\")\n",
        "total_cleaned = 0\n",
        "voice_dirs = [d for d in sorted(os.listdir(VOICES_DIR)) if os.path.isdir(os.path.join(VOICES_DIR, d))]\n",
        "\n",
        "if not voice_dirs:\n",
        "    print(\"‚ö†Ô∏è No subfolders found in voices directory. Add at least one voice folder.\")\n",
        "else:\n",
        "    for voice in voice_dirs:\n",
        "        vdir = os.path.join(VOICES_DIR, voice)\n",
        "        # Original WAVs that don't already have a _clean version\n",
        "        originals = [f for f in glob.glob(os.path.join(vdir, \"*.wav\")) if not f.endswith(\"_clean.wav\")]\n",
        "        if not originals:\n",
        "            print(f\"‚òëÔ∏è Nothing new to clean in: {vdir}\")\n",
        "            continue\n",
        "\n",
        "        for wav in originals:\n",
        "            out = wav[:-4] + \"_clean.wav\"\n",
        "            cmd = f'''ffmpeg -y -i \"{wav}\" -ac 1 -ar {TARGET_SR} \\\n",
        "-af \"highpass=f=80,lowpass=f=11000,\\\n",
        "silenceremove=start_periods=1:start_silence=0.4:start_threshold=-40dB,\\\n",
        "loudnorm=I=-18:TP=-1.0:LRA=11\" \"{out}\"'''\n",
        "            # Run, but don't crash the whole cell if one file fails\n",
        "            try:\n",
        "                subprocess.run(shlex.split(cmd), check=True, capture_output=True)\n",
        "                total_cleaned += 1\n",
        "                print(f\"‚úÖ Cleaned ‚Üí {out}\")\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"‚ùå FFmpeg failed for {wav}: {e.stderr.decode('utf-8', errors='ignore')[:300]}\")\n",
        "\n",
        "print(f\"üéØ Cleaning complete. New files created: {total_cleaned}\")\n",
        "print(\"‚ÑπÔ∏è Your backend will prefer *_clean.wav files if present.\")"
      ],
      "metadata": {
        "id": "ZHecCByJKMP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# STEP 3 ‚Äî Start the Backend Voice Cloning API\n",
        "# ====================================================================\n",
        "import sys, os, re, base64, torch, torchaudio, unicodedata\n",
        "from pathlib import Path\n",
        "from threading import Thread\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from werkzeug.serving import run_simple\n",
        "\n",
        "# --- CSM imports ---\n",
        "sys.path.append('/content/csm')\n",
        "from generator import load_csm_1b, Segment\n",
        "\n",
        "# -----------------------------\n",
        "# Config & Constants\n",
        "# -----------------------------\n",
        "SAMPLE_RATE = 24000\n",
        "VOICES_DIR  = Path(\"/content/drive/MyDrive/My_Voice_API_Files/voices\")\n",
        "\n",
        "VOICE_TRANSCRIPTS = {\n",
        "    \"MyVoice\":   \"The quick brown fox jumps over the lazy dog; that is a fact. Should we chase those azure clouds and judge their graceful, quiet movement? For my voice to be cloned with vision and expertise, I must speak this very sentence. My name is Alvin Moyo?\",\n",
        "    \"MyVoice2\":  \"What does it take to build an AI that truly understands a resume? I am Alvin Moyo; in this video I will walk you through the journey of how I built and evolved a resume ranking pipeline. Starting traditional machine learning and pushing towards something smarter.\",\n",
        "    \"SemihVoice\":\"My name is Semih; I am the Director of AI at Apziva. We are very pleased to have you with us today. Today we have Chris Turner with us. Chris is an AI expert and, also an AI resident at Apziva. A background in genetics and biology with years of experience in\"\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Load model\n",
        "# -----------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Using device: {DEVICE}\")\n",
        "print(\"üß† Loading CSM model...\")\n",
        "generator = load_csm_1b(device=DEVICE)\n",
        "print(\"‚úÖ Model loaded.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Build voice anchors (prefer *_clean.wav; ensure mono + 24k)\n",
        "# -----------------------------\n",
        "if not VOICES_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Voices directory not found: {VOICES_DIR}\")\n",
        "\n",
        "VOICE_PROMPTS = {}\n",
        "print(\"üîä Loading voice prompts...\")\n",
        "for voice_folder in sorted(p for p in VOICES_DIR.iterdir() if p.is_dir()):\n",
        "    try:\n",
        "        clean = sorted(voice_folder.glob(\"*_clean.wav\"))\n",
        "        raw   = sorted([p for p in voice_folder.glob(\"*.wav\") if not p.name.endswith(\"_clean.wav\")])\n",
        "        if not (clean or raw):\n",
        "            print(f\"‚ö†Ô∏è No .wav files in '{voice_folder.name}' ‚Äî skipping.\")\n",
        "            continue\n",
        "        prompt_file = (clean or raw)[0]\n",
        "\n",
        "        audio, sr = torchaudio.load(prompt_file)\n",
        "        if audio.shape[0] > 1:\n",
        "            audio = audio.mean(dim=0, keepdim=True)\n",
        "        if sr != SAMPLE_RATE:\n",
        "            audio = torchaudio.functional.resample(audio, orig_freq=sr, new_freq=SAMPLE_RATE)\n",
        "\n",
        "        prompt_text = VOICE_TRANSCRIPTS.get(voice_folder.name, \"A generic prompt.\")\n",
        "        VOICE_PROMPTS[voice_folder.name] = Segment(\n",
        "            speaker=0, text=prompt_text, audio=audio.squeeze(0).contiguous()\n",
        "        )\n",
        "        print(f\"‚úÖ Loaded '{voice_folder.name}' ‚Üí {prompt_file.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading voice '{voice_folder.name}': {e}\")\n",
        "\n",
        "# Warmup (optional)\n",
        "if VOICE_PROMPTS:\n",
        "    any_anchor = next(iter(VOICE_PROMPTS.values()))\n",
        "    with torch.inference_mode():\n",
        "        _ = generator.generate(text=\"Hello.\", speaker=0, context=[any_anchor],\n",
        "                               max_audio_length_ms=1500, temperature=0.7)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No voices loaded. /generate will return 503 until voices are available.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def split_into_sentences(text: str):\n",
        "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text.strip()) if s.strip()]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_long_form_speech(text: str, anchor: Segment, wps: float, temp: float):\n",
        "    text = unicodedata.normalize(\"NFKC\", text or \"\").strip()\n",
        "    sentences = split_into_sentences(text)[:20]  # cap to 20 sentences for demo safety\n",
        "    if not sentences:\n",
        "        return torch.zeros(SAMPLE_RATE // 2, dtype=torch.float32)\n",
        "\n",
        "    wps  = float(max(1.2, min(wps, 3.0)))\n",
        "    temp = float(max(0.2, min(temp, 1.0)))\n",
        "\n",
        "    parts = []\n",
        "    pause = torch.zeros(int(0.25 * SAMPLE_RATE), dtype=torch.float32)\n",
        "    for s in sentences:\n",
        "        max_ms = int(max(3500, (len(s.split()) / wps) * 1000 * 1.2))\n",
        "        chunk = generator.generate(text=s, speaker=0, context=[anchor],\n",
        "                                   max_audio_length_ms=max_ms, temperature=temp)\n",
        "        chunk = chunk.to(dtype=torch.float32).contiguous().cpu()\n",
        "        parts.extend([chunk, pause])\n",
        "\n",
        "    final = torch.cat(parts[:-1]) if len(parts) > 1 else parts[0]\n",
        "    peak = final.abs().max().item()\n",
        "    if peak > 1e-6:\n",
        "        final = final / peak\n",
        "    final = torch.clamp(final * 1.2, -1.0, 1.0)  # gentle lift\n",
        "    return final\n",
        "\n",
        "# -----------------------------\n",
        "# Flask API\n",
        "# -----------------------------\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        \"status\": \"ok\",\n",
        "        \"device\": DEVICE,\n",
        "        \"voices\": sorted(list(VOICE_PROMPTS.keys())),\n",
        "        \"sample_rate\": SAMPLE_RATE\n",
        "    })\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_endpoint():\n",
        "    if not VOICE_PROMPTS:\n",
        "        return jsonify({\"error\": \"No voices available on server. Please add prompts and reload.\"}), 503\n",
        "\n",
        "    data = request.get_json(silent=True) or {}\n",
        "    text = (data.get(\"text\") or \"\").strip()\n",
        "    if not text:\n",
        "        return jsonify({\"error\": \"Missing 'text'\"}), 400\n",
        "    if len(text) > 600:\n",
        "        return jsonify({\"error\": \"Text too long for demo; please shorten to ‚â§600 chars.\"}), 400\n",
        "\n",
        "    voice_name = data.get(\"voice\", \"MyVoice\")\n",
        "    if voice_name not in VOICE_PROMPTS:\n",
        "        return jsonify({\"error\": f\"Voice '{voice_name}' not found.\"}), 400\n",
        "\n",
        "    wps  = float(data.get(\"words_per_sec\", 2.0))\n",
        "    temp = float(data.get(\"temperature\", 0.75))\n",
        "\n",
        "    audio = generate_long_form_speech(text, VOICE_PROMPTS[voice_name], wps, temp)\n",
        "    audio = audio.unsqueeze(0)  # (1, T)\n",
        "\n",
        "    torchaudio.save(\"output.wav\", audio, SAMPLE_RATE, encoding=\"PCM_S\", bits_per_sample=16)\n",
        "    with open(\"output.wav\", \"rb\") as f:\n",
        "        encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "    os.remove(\"output.wav\")\n",
        "\n",
        "    return jsonify({\"audio_data\": encoded, \"sample_rate\": SAMPLE_RATE, \"mime\": \"audio/wav\"})\n",
        "\n",
        "# -----------------------------\n",
        "# Run server (threaded for notebooks)\n",
        "# -----------------------------\n",
        "def run_server():\n",
        "    run_simple(\"0.0.0.0\", 8000, app, threaded=True)\n",
        "\n",
        "Thread(target=run_server, daemon=True).start()\n",
        "print(\"üöÄ Backend API Server is now running on :8000\")"
      ],
      "metadata": {
        "id": "cuEvVNflqvBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# STEP 4 ‚Äî Launch the Gradio Conversational App (GPT Brain & Advanced Tuning)\n",
        "# ====================================================================\n",
        "import os, base64, requests, gradio as gr\n",
        "import openai\n",
        "\n",
        "# --- 1) OpenAI client (env first, Colab Secrets fallback) ---\n",
        "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    try:\n",
        "        from google.colab import userdata  # only works in Colab\n",
        "        api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    except Exception:\n",
        "        api_key = None\n",
        "if not api_key:\n",
        "    raise RuntimeError(\"OPENAI_API_KEY not found. Set it in environment or Colab Secrets.\")\n",
        "openai_client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# --- 2) Config ---\n",
        "VOICE_CLONE_URL = \"http://127.0.0.1:8000/generate\"\n",
        "HEALTH_URL      = \"http://127.0.0.1:8000/health\"\n",
        "\n",
        "ASR_PRIMARY  = \"gpt-4o-mini-transcribe\"  # STT (primary)\n",
        "ASR_FALLBACK = \"whisper-1\"               # STT (fallback)\n",
        "GPT_MODEL    = \"gpt-4o\"                  # Brain\n",
        "\n",
        "# --- 3) Resolve voice list (prefer in-notebook VOICE_PROMPTS; else /health) ---\n",
        "if \"VOICE_PROMPTS\" in globals() and isinstance(VOICE_PROMPTS, dict) and VOICE_PROMPTS:\n",
        "    VOICE_CHOICES = sorted(list(VOICE_PROMPTS.keys()))\n",
        "else:\n",
        "    try:\n",
        "        resp = requests.get(HEALTH_URL, timeout=5)\n",
        "        VOICE_CHOICES = resp.json().get(\"voices\", []) if resp.ok else []\n",
        "    except Exception:\n",
        "        VOICE_CHOICES = []\n",
        "if not VOICE_CHOICES:\n",
        "    VOICE_CHOICES = [\"MyVoice\"]  # fallback placeholder\n",
        "DEFAULT_VOICE = VOICE_CHOICES[0]\n",
        "\n",
        "# --- 4) Core helpers ---\n",
        "def transcribe_audio(mic_path: str) -> str:\n",
        "    \"\"\"Transcribe microphone audio with 4o-mini-transcribe; fallback to whisper-1.\"\"\"\n",
        "    if not mic_path:\n",
        "        return \"\"\n",
        "    try:\n",
        "        with open(mic_path, \"rb\") as f:\n",
        "            r = openai_client.audio.transcriptions.create(model=ASR_PRIMARY, file=f)\n",
        "        return (getattr(r, \"text\", None) or str(r)).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[ASR primary failed: {e}] Falling back to {ASR_FALLBACK}...\")\n",
        "        try:\n",
        "            with open(mic_path, \"rb\") as f:\n",
        "                r = openai_client.audio.transcriptions.create(model=ASR_FALLBACK, file=f, response_format=\"text\")\n",
        "            return str(r).strip()\n",
        "        except Exception as e2:\n",
        "            print(f\"[ASR fallback failed: {e2}]\")\n",
        "            return \"\"\n",
        "\n",
        "def get_gpt_response(user_text: str) -> str:\n",
        "    if not user_text:\n",
        "        return \"\"\n",
        "    try:\n",
        "        out = openai_client.chat.completions.create(\n",
        "            model=GPT_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful, concise assistant. Keep replies to 1‚Äì2 short sentences.\"},\n",
        "                {\"role\": \"user\", \"content\": user_text}\n",
        "            ],\n",
        "            temperature=0.5,\n",
        "            max_tokens=120\n",
        "        )\n",
        "        return out.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[GPT error] {e}\")\n",
        "        return \"Sorry, I hit an error generating a reply.\"\n",
        "\n",
        "def tts_request(text: str, voice: str, wps: float, temp: float) -> tuple[str|None, str]:\n",
        "    \"\"\"Call your local Flask TTS API and return (audio_path, message).\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return None, \"Please provide some text.\"\n",
        "    payload = {\"text\": text.strip(), \"voice\": voice, \"words_per_sec\": float(wps), \"temperature\": float(temp)}\n",
        "    try:\n",
        "        resp = requests.post(VOICE_CLONE_URL, json=payload, timeout=(9, 60))\n",
        "        if resp.status_code != 200:\n",
        "            return None, f\"TTS error: HTTP {resp.status_code} ‚Äî {resp.text}\"\n",
        "        data = resp.json()\n",
        "        b = base64.b64decode(data[\"audio_data\"])\n",
        "        out_path = \"ai_voice.wav\"\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            f.write(b)\n",
        "        return out_path, text.strip()\n",
        "    except requests.Timeout:\n",
        "        return None, \"TTS timeout: generation took too long.\"\n",
        "    except Exception as e:\n",
        "        return None, f\"TTS exception: {e}\"\n",
        "\n",
        "# --- 5) Gradio handlers ---\n",
        "def talk_to_brain(mic, voice, wps, temp):\n",
        "    \"\"\"Mic ‚Üí Transcribe ‚Üí GPT ‚Üí TTS\"\"\"\n",
        "    transcript = transcribe_audio(mic)\n",
        "    if not transcript:\n",
        "        return None, \"I couldn't hear that‚Äîplease try again.\"\n",
        "    reply = get_gpt_response(transcript)\n",
        "    wav, msg = tts_request(reply, voice, wps, temp)\n",
        "    return wav, (reply if wav else msg)\n",
        "\n",
        "def type_to_voice(text, voice, wps, temp):\n",
        "    \"\"\"Type ‚Üí TTS (reads exactly what was typed)\"\"\"\n",
        "    wav, msg = tts_request(text, voice, wps, temp)\n",
        "    return wav, msg\n",
        "\n",
        "# --- 6) UI: two tabs, one app ---\n",
        "with gr.Blocks(title=\"Real-Time Conversational Voice Cloning\") as app:\n",
        "    gr.Markdown(\"### Real-Time Conversational Voice Cloning\\nSpeak or type, pick a voice, and hear the AI reply.\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Tab 1: Talk to Me\n",
        "        with gr.Tab(\"üé§ Talk to Me\"):\n",
        "            mic = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Hold to record, then release\")\n",
        "            with gr.Row():\n",
        "                voice1 = gr.Dropdown(VOICE_CHOICES, value=DEFAULT_VOICE, label=\"Choose AI Voice\")\n",
        "                wps1   = gr.Slider(1.5, 3.0, value=2.0, step=0.1, label=\"Words Per Second (Speed)\")\n",
        "                temp1  = gr.Slider(0.2, 1.0, value=0.75, step=0.05, label=\"Temperature (Creativity)\")\n",
        "            out_audio1 = gr.Audio(label=\"AI Response (Audio)\")\n",
        "            out_text1  = gr.Textbox(label=\"AI Reply (Text)\", lines=3)\n",
        "            gr.Button(\"Respond\").click(talk_to_brain, [mic, voice1, wps1, temp1], [out_audio1, out_text1])\n",
        "\n",
        "        # Tab 2: Type to Speak\n",
        "        with gr.Tab(\"‚å®Ô∏è Type to Speak\"):\n",
        "            txt = gr.Textbox(label=\"Type something for the AI to speak\", lines=3,\n",
        "                             placeholder=\"e.g., Welcome to my workshop!\")\n",
        "            with gr.Row():\n",
        "                voice2 = gr.Dropdown(VOICE_CHOICES, value=DEFAULT_VOICE, label=\"Choose AI Voice\")\n",
        "                wps2   = gr.Slider(1.5, 3.0, value=2.0, step=0.1, label=\"Words Per Second (Speed)\")\n",
        "                temp2  = gr.Slider(0.2, 1.0, value=0.75, step=0.05, label=\"Temperature (Creativity)\")\n",
        "            out_audio2 = gr.Audio(label=\"Synthesized Audio\")\n",
        "            out_text2  = gr.Textbox(label=\"(Echo) Text Sent\", lines=2)\n",
        "            gr.Button(\"Speak\").click(type_to_voice, [txt, voice2, wps2, temp2], [out_audio2, out_text2])\n",
        "\n",
        "    gr.Markdown(\"> Tip: If first audio is slow, that's model warmup. Subsequent runs are faster.\")\n",
        "\n",
        "app.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "iBmbEYH3vyI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=================================================================================================================\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tuSbGS-_ds3x"
      }
    }
  ]
}