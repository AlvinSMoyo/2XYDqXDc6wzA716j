# MonReader Cognitive Engine: A Multiâ€‘Modal AI Pipeline

> **Turn a physical Shona hymnbook into an interactive, accessible, and audible experience.**

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](#) [![Colab](https://img.shields.io/badge/Run%20on-Google%20Colab-orange)](#) [![Torch](https://img.shields.io/badge/PyTorch-%F0%9F%94%A5-red)](#)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlvinSMoyo/2XYDqXDc6wzA716j/blob/main/notebooks/monreader_cognitive_engine.ipynb) [![Open API Notebook in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlvinSMoyo/2XYDqXDc6wzA716j/blob/main/notebooks/MonReader_Clone_Your_Voice_Flask_API.ipynb)

MonReader is a sixâ€‘phase, endâ€‘toâ€‘end AI system spanning **Computer Vision (CNN)**, **OCR**, **NLP**, and **Textâ€‘toâ€‘Speech**. The engine detects page turns, extracts text from images, understands crossâ€‘lingual meaning, and finally **speaks** the content in a customâ€‘cloned Shona voice.

---

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](#) [![Issues](https://img.shields.io/github/issues/AlvinSMoyo/2XYDqXDc6wzA716j.svg)](https://github.com/AlvinSMoyo/2XYDqXDc6wzA716j/issues) [![Last Commit](https://img.shields.io/github/last-commit/AlvinSMoyo/2XYDqXDc6wzA716j.svg)](https://github.com/AlvinSMoyo/2XYDqXDc6wzA716j/commits/main) [![Stars](https://img.shields.io/github/stars/AlvinSMoyo/2XYDqXDc6wzA716j.svg?style=social)](https://github.com/AlvinSMoyo/2XYDqXDc6wzA716j/stargazers)

## ğŸ”­ Project Overview

The mission was to build a complete pipeline that could:

1. **Visually detect** page turns to trigger a scanner.
2. **Accurately extract** Shona text from scanned images.
3. **Analyze and align** meaning across Shona â‡„ English.
4. **Synthesize** highâ€‘quality audio in a customâ€‘cloned Shona voice.

This was accomplished over six distinct phases, each building upon the last.

---

## ğŸ—ºï¸ The Six Phases of Development

### ğŸ“¸ Phase 1 â€” Image Classification Pipeline

**Essence:** An endâ€‘toâ€‘end **image classification** project to flawlessly distinguish a page being turned (**flip**) vs held steady (**notflip**)â€”the trigger for scanning.

**Highlights**

* **Transfer Learning with CNNs:** A rigorous bakeâ€‘off across leading architectures.
* **AlvinNet + Lion Optimizer:** A lightweight, custom CNN refined with **Lion**.
* **Performance & Explainability:** Nearâ€‘perfect **F1 score**; **Gradâ€‘CAM** verified feature focus.

---

### ğŸ“– Phase 2 â€” From Scanned Page to Digital Text (OCR on the Shona Hymnal)

**Essence:** A focused **OCR** challenge on highâ€‘resolution ELCZ Shona hymnbook images.

**Highlights**

* **Competitive OCR:** **EasyOCR** vs **Tesseract** headâ€‘toâ€‘head.
* **Quantitative Evaluation:** Winner selected via **Levenshtein distance**.

---

### ğŸ§  Phase 3 â€” Hybrid AI for Hymn Transcription & Analysis

**Essence:** Beyond standalone OCRâ€”build a **Hybrid AI Pipeline** comparing **Visionâ€‘Language Models (VLMs)** against a twoâ€‘step flow where a **PLM** corrects OCR.

**Highlights**

* **Modern Model Bakeâ€‘Off:** VLMs reading text from images vs OCRâ†’PLM correction.
* **Adjudication:** **Google Vision** + **GPTâ€‘4** for quality judgments.
* **Thematic & Liturgical Insight:** Pipeline scaffolding for **context**, **themes**, and **translation checks**.

---

### â†”ï¸ Phase 4 â€” Crossâ€‘Lingual Alignment via Semantic Similarity

**Essence:** Add an **NLP** layer to validate translations by measuring Shonaâ€“English **semantic similarity**.

**Highlights**

* **Vector Representations:** Sentence embeddings encode meaning as vectors.
* **Distanceâ€‘Based Validation:** Smaller vector distance â‡’ stronger alignment.

---

### ğŸ—£ï¸ Phase 5 â€” Case Study in Textâ€‘toâ€‘Speech (TTS) Evaluation

**Essence:** A **comparative TTS study** to choose the best base model for Shona speech synthesis.

**Highlights**

* **Comparative Evaluation:** Multiple TTS models assessed for clarity, naturalness, and pronunciation.
* **Humanâ€‘Centric Judgment:** Final pick based on listening tests and usability for Shona.

---

### ğŸ¶ Phase 6 â€” Enhanced Inference Conditioning for Shona Voice Cloning

**Essence:** A **trainingâ€‘free** strategy using **CSMâ€‘1B** with **inferenceâ€‘time conditioning** on a curated reference voice bank (no model retraining required).

**Highlights**

* **Prompt + Reference Bank:** Short anchor prompt + curated clips reinforce accent/timbre.
* **Rapid Prototyping:** Lowâ€‘cost gains in pronunciation and style.

---

## ğŸ“¦ Dataset (Phase 1)

* **Goal:** Predict whether a single image represents a **page flip** or **not flip**.
* **Primary Metric:** **F1 score** (higher is better).
* **Where to put data (Colab):** mount Drive and place images under a folder like `/content/drive/MyDrive/MonReader/data/phase1/`.

---

## âš™ï¸ How to Run (Colabâ€‘First)

This project is designed for **Google Colab** with GPU enabled.

### âœ… Prerequisites

* Google account + **Colab** access (Pro recommended)
* Project files + reference audio in **Google Drive**
* **Hugging Face** account + access token with read permissions

### ğŸš€ Setup & Execution

1. **Clone the repository**

   ```bash
   git clone https://github.com/AlvinSMoyo/2XYDqXDc6wzA716j.git
   cd 2XYDqXDc6wzA716j
   ```
2. **Open the Notebook in Colab**
   Click this badge/link: [Open in Colab](https://colab.research.google.com/github/AlvinSMoyo/2XYDqXDc6wzA716j/blob/main/notebooks/monreader_cognitive_engine.ipynb).
3. **Organize Your Files**
   Place voice prompts/reference clips in your Drive using the expected folder structure (see **Reference Audio** notes in the notebook).
4. **Set Colab Secrets**
   Click the **ğŸ”‘ (Secrets)** icon in the left sidebar and add `HF_TOKEN` with your Hugging Face token.
5. **Run Cells in Order**

   * **Step 6.0 â€” Mount Drive**
   * **Step 6.1 â€” Environment Setup** (installs PyTorch, torchaudio, Whisper, etc.)
   * **Step 6.2 â€” Prepare Reference Audio** (anchor prompt)
   * **Step 6.4 â€” Build Reference Bank** (curated clips library)
   * **Step 6.6 â€” Final Generation** (baseline vs bankâ€‘boosted outputs)

> Outputs are saved to Colab and previewed inline. The notebook also prints save locations.

---

## ğŸ”Œ Local API (Flask)

Run MonReader as a small HTTP service to generate audio from text.

**Option A â€” Colab (recommended)**
Open **[MonReader\_Clone\_Your\_Voice\_Flask\_API.ipynb](notebooks/MonReader_Clone_Your_Voice_Flask_API.ipynb)** and run **Step 3 â€“ Start the Backend API** (exposes `/health` and `/generate` on `http://127.0.0.1:8000`). The `/generate` endpoint returns **base64 WAV** in JSON.

**Option B â€” Local script (if you have `app/app.py`)**

```bash
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r app/requirements.txt
python app/app.py  # http://127.0.0.1:8000
```

**Quick test**

```bash
curl -X POST http://127.0.0.1:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"text":"MonReader says hello in Shona.","mode":"bank"}'
```

> Full API docs â†’ see **[app/README.md](app/README.md)**.

---

## ğŸ§© Example Folder Structure (Guidance)

```
<repo-root>/
â”œâ”€ notebooks/
â”‚  â”œâ”€ monreader_cognitive_engine.ipynb
â”‚  â””â”€ MonReader_Clone_Your_Voice_Flask_API.ipynb
â”œâ”€ app/
â”‚  â””â”€ README.md                 # API docs (Colab-first)
â”œâ”€ reference_audio/             # optional local samples (Drive path preferred)
â”œâ”€ outputs/                     # generated audio (gitignored)
â”œâ”€ .gitignore
â”œâ”€ README.md
â””â”€ LICENSE
```

> Note: Exact paths in the notebook point to **Google Drive**. Keep Drive and repo paths consistent.

---

## ğŸ“¦ Key Dependencies

* Python **3.10+**
* **PyTorch**, **torchaudio**
* **OpenAI Whisper** (transcription)
* **Flask** / **Gradio** (optional demos)
* Plus others pinned in the notebook install cell

**Install locally (optional)**

```bash
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r app/requirements.txt  # if provided
```

---

## ğŸ§ª Results & Notes

* Phase 1 achieved **nearâ€‘perfect F1** and interpretable Gradâ€‘CAM heatmaps.
* OCR winner selected via **Levenshtein** distance.
* Hybrid (VLM vs OCRâ†’PLM) adjudicated with **Google Vision** + **GPTâ€‘4**.
* Crossâ€‘lingual alignment validated using **sentence embeddings**.
* TTS shortlisted via listening tests; **CSMâ€‘1B** conditioning provided fast voiceâ€‘clone gains without fineâ€‘tuning.

---

## ğŸ” Secrets & Tokens

* Store **`HF_TOKEN`** in Colab Secrets (not in code).
* Do **not** commit tokens to Git.

---

## ğŸ“ Citation & Acknowledgments

* Thanks to openâ€‘source communities behind **PyTorch**, **Whisper**, **CSMâ€‘1B**, and OCR libraries.
* Hymnbook content used for research and accessibility purposes.

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE) for details.

---

## ğŸ™Œ Contributing

PRs that improve docs, robustness, and language support are welcome. For larger changes, please open an issue first to discuss direction.



